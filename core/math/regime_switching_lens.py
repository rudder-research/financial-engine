"""
Regime Switching Models Lens
============================
Identifies distinct market/system states and transitions between them,
capturing nonlinear dynamics that standard models miss.

Key Applications:
- Bull/bear market detection
- Economic expansion/contraction cycles
- Climate regime identification (El Niño, La Niña, neutral)

Methods:
- Hidden Markov Models (HMM): Probabilistic state inference
- Markov Switching Dynamic Regression (MSDR): Regime-dependent parameters

Note: This is a pure NumPy/SciPy implementation without external HMM libraries.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
from scipy.special import logsumexp
from scipy.optimize import minimize
import warnings


@dataclass
class RegimeState:
    """Represents a regime/state in the model."""
    id: int
    mean: float
    variance: float
    label: Optional[str] = None


@dataclass
class HMMResult:
    """Container for HMM analysis results."""
    states: np.ndarray           # Most likely state sequence
    state_probs: np.ndarray      # Probability of each state at each time
    transition_matrix: np.ndarray
    state_means: np.ndarray
    state_variances: np.ndarray
    log_likelihood: float
    aic: float
    bic: float


class HiddenMarkovModel:
    """
    Gaussian Hidden Markov Model for regime detection.
    
    Assumes observations are generated by a hidden Markov process
    where each hidden state has its own Gaussian emission distribution.
    """
    
    def __init__(self, n_states: int = 2, max_iter: int = 100, tol: float = 1e-4):
        """
        Initialize HMM.
        
        Parameters
        ----------
        n_states : int
            Number of hidden states/regimes
        max_iter : int
            Maximum EM iterations
        tol : float
            Convergence tolerance
        """
        self.n_states = n_states
        self.max_iter = max_iter
        self.tol = tol
        
        # Parameters to be estimated
        self.initial_probs = None
        self.transition_matrix = None
        self.means = None
        self.variances = None
        
        self._fitted = False
    
    def _initialize_parameters(self, data: np.ndarray) -> None:
        """Initialize parameters using k-means style initialization."""
        n = len(data)
        
        # Initial state probabilities (uniform)
        self.initial_probs = np.ones(self.n_states) / self.n_states
        
        # Transition matrix (slightly favor staying in same state)
        self.transition_matrix = np.ones((self.n_states, self.n_states))
        np.fill_diagonal(self.transition_matrix, 2)
        self.transition_matrix /= self.transition_matrix.sum(axis=1, keepdims=True)
        
        # Initialize means using percentiles
        percentiles = np.linspace(10, 90, self.n_states)
        self.means = np.percentile(data, percentiles)
        
        # Initialize variances
        overall_var = np.var(data)
        self.variances = np.full(self.n_states, overall_var / self.n_states)
    
    def _log_emission_probs(self, data: np.ndarray) -> np.ndarray:
        """Compute log emission probabilities for each state."""
        n = len(data)
        log_probs = np.zeros((n, self.n_states))
        
        for k in range(self.n_states):
            # Log of Gaussian PDF
            log_probs[:, k] = (
                -0.5 * np.log(2 * np.pi * self.variances[k])
                - 0.5 * (data - self.means[k])**2 / self.variances[k]
            )
        
        return log_probs
    
    def _forward(self, log_emission: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        Forward algorithm (alpha pass).
        
        Computes P(observations up to time t, state at time t).
        """
        n = log_emission.shape[0]
        log_alpha = np.zeros((n, self.n_states))
        
        # Initialize
        log_alpha[0] = np.log(self.initial_probs + 1e-10) + log_emission[0]
        
        # Forward pass
        log_trans = np.log(self.transition_matrix + 1e-10)
        
        for t in range(1, n):
            for j in range(self.n_states):
                log_alpha[t, j] = logsumexp(log_alpha[t-1] + log_trans[:, j]) + log_emission[t, j]
        
        log_likelihood = logsumexp(log_alpha[-1])
        
        return log_alpha, log_likelihood
    
    def _backward(self, log_emission: np.ndarray) -> np.ndarray:
        """
        Backward algorithm (beta pass).
        
        Computes P(observations after time t | state at time t).
        """
        n = log_emission.shape[0]
        log_beta = np.zeros((n, self.n_states))
        
        # Initialize (log(1) = 0)
        log_beta[-1] = 0
        
        # Backward pass
        log_trans = np.log(self.transition_matrix + 1e-10)
        
        for t in range(n - 2, -1, -1):
            for i in range(self.n_states):
                log_beta[t, i] = logsumexp(
                    log_trans[i, :] + log_emission[t+1] + log_beta[t+1]
                )
        
        return log_beta
    
    def _e_step(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
        """
        E-step: Compute posterior state probabilities.
        
        Returns gamma (state probabilities) and xi (transition probabilities).
        """
        log_emission = self._log_emission_probs(data)
        log_alpha, log_likelihood = self._forward(log_emission)
        log_beta = self._backward(log_emission)
        
        n = len(data)
        
        # Gamma: P(state at time t | all observations)
        log_gamma = log_alpha + log_beta
        log_gamma -= logsumexp(log_gamma, axis=1, keepdims=True)
        gamma = np.exp(log_gamma)
        
        # Xi: P(state i at t, state j at t+1 | all observations)
        xi = np.zeros((n - 1, self.n_states, self.n_states))
        log_trans = np.log(self.transition_matrix + 1e-10)
        
        for t in range(n - 1):
            for i in range(self.n_states):
                for j in range(self.n_states):
                    xi[t, i, j] = np.exp(
                        log_alpha[t, i] + log_trans[i, j] + 
                        log_emission[t+1, j] + log_beta[t+1, j] - log_likelihood
                    )
        
        return gamma, xi, log_likelihood
    
    def _m_step(self, data: np.ndarray, gamma: np.ndarray, xi: np.ndarray) -> None:
        """M-step: Update parameters based on posterior probabilities."""
        n = len(data)
        
        # Update initial probabilities
        self.initial_probs = gamma[0] / gamma[0].sum()
        
        # Update transition matrix
        for i in range(self.n_states):
            for j in range(self.n_states):
                self.transition_matrix[i, j] = xi[:, i, j].sum() / gamma[:-1, i].sum()
        
        # Normalize rows
        self.transition_matrix /= self.transition_matrix.sum(axis=1, keepdims=True)
        
        # Update means and variances
        for k in range(self.n_states):
            gamma_sum = gamma[:, k].sum()
            
            # Mean
            self.means[k] = (gamma[:, k] * data).sum() / gamma_sum
            
            # Variance
            diff_sq = (data - self.means[k])**2
            self.variances[k] = (gamma[:, k] * diff_sq).sum() / gamma_sum
            
            # Ensure minimum variance
            self.variances[k] = max(self.variances[k], 1e-6)
    
    def fit(self, data: np.ndarray) -> 'HiddenMarkovModel':
        """
        Fit HMM using Baum-Welch (EM) algorithm.
        
        Parameters
        ----------
        data : np.ndarray
            1D array of observations
            
        Returns
        -------
        self
        """
        data = np.asarray(data).flatten()
        self._initialize_parameters(data)
        
        prev_ll = -np.inf
        
        for iteration in range(self.max_iter):
            # E-step
            gamma, xi, log_likelihood = self._e_step(data)
            
            # Check convergence
            if abs(log_likelihood - prev_ll) < self.tol:
                break
            prev_ll = log_likelihood
            
            # M-step
            self._m_step(data, gamma, xi)
        
        self._fitted = True
        self._log_likelihood = log_likelihood
        self._n_obs = len(data)
        
        return self
    
    def predict_states(self, data: np.ndarray) -> np.ndarray:
        """
        Predict most likely state sequence using Viterbi algorithm.
        
        Parameters
        ----------
        data : np.ndarray
            Observations
            
        Returns
        -------
        np.ndarray
            Most likely state at each time
        """
        if not self._fitted:
            raise ValueError("Model must be fitted first")
        
        data = np.asarray(data).flatten()
        n = len(data)
        log_emission = self._log_emission_probs(data)
        log_trans = np.log(self.transition_matrix + 1e-10)
        
        # Viterbi algorithm
        log_delta = np.zeros((n, self.n_states))
        psi = np.zeros((n, self.n_states), dtype=int)
        
        # Initialize
        log_delta[0] = np.log(self.initial_probs + 1e-10) + log_emission[0]
        
        # Forward pass
        for t in range(1, n):
            for j in range(self.n_states):
                temp = log_delta[t-1] + log_trans[:, j]
                psi[t, j] = np.argmax(temp)
                log_delta[t, j] = temp[psi[t, j]] + log_emission[t, j]
        
        # Backtrack
        states = np.zeros(n, dtype=int)
        states[-1] = np.argmax(log_delta[-1])
        
        for t in range(n - 2, -1, -1):
            states[t] = psi[t + 1, states[t + 1]]
        
        return states
    
    def predict_proba(self, data: np.ndarray) -> np.ndarray:
        """
        Get probability of each state at each time.
        
        Parameters
        ----------
        data : np.ndarray
            Observations
            
        Returns
        -------
        np.ndarray
            Shape (n_samples, n_states) probability matrix
        """
        if not self._fitted:
            raise ValueError("Model must be fitted first")
        
        data = np.asarray(data).flatten()
        log_emission = self._log_emission_probs(data)
        log_alpha, _ = self._forward(log_emission)
        log_beta = self._backward(log_emission)
        
        log_gamma = log_alpha + log_beta
        log_gamma -= logsumexp(log_gamma, axis=1, keepdims=True)
        
        return np.exp(log_gamma)
    
    def get_results(self, data: np.ndarray) -> HMMResult:
        """
        Get comprehensive results.
        
        Parameters
        ----------
        data : np.ndarray
            Observations (same used for fitting)
            
        Returns
        -------
        HMMResult
            Complete analysis results
        """
        if not self._fitted:
            raise ValueError("Model must be fitted first")
        
        states = self.predict_states(data)
        state_probs = self.predict_proba(data)
        
        # Model selection criteria
        n_params = (self.n_states - 1 +                    # Initial probs
                   self.n_states * (self.n_states - 1) +   # Transition matrix
                   2 * self.n_states)                       # Means and variances
        
        aic = -2 * self._log_likelihood + 2 * n_params
        bic = -2 * self._log_likelihood + n_params * np.log(self._n_obs)
        
        return HMMResult(
            states=states,
            state_probs=state_probs,
            transition_matrix=self.transition_matrix,
            state_means=self.means,
            state_variances=self.variances,
            log_likelihood=self._log_likelihood,
            aic=aic,
            bic=bic
        )


class MarkovSwitchingRegression:
    """
    Markov Switching Dynamic Regression (Hamilton-style).
    
    Models time series where the parameters (mean, variance, AR coefficients)
    switch between regimes according to a hidden Markov chain.
    
    Y_t = μ(S_t) + φ(S_t) * Y_{t-1} + σ(S_t) * ε_t
    
    where S_t follows a Markov chain.
    """
    
    def __init__(
        self,
        n_regimes: int = 2,
        ar_order: int = 1,
        switching_ar: bool = True,
        max_iter: int = 100,
        tol: float = 1e-4
    ):
        """
        Initialize Markov Switching model.
        
        Parameters
        ----------
        n_regimes : int
            Number of regimes
        ar_order : int
            Autoregressive order (0 for just switching mean/variance)
        switching_ar : bool
            If True, AR coefficients also switch with regime
        max_iter : int
            Maximum EM iterations
        tol : float
            Convergence tolerance
        """
        self.n_regimes = n_regimes
        self.ar_order = ar_order
        self.switching_ar = switching_ar
        self.max_iter = max_iter
        self.tol = tol
        
        self._fitted = False
    
    def _build_design_matrix(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Build X and Y matrices for regression."""
        n = len(data)
        
        if self.ar_order == 0:
            return np.ones((n, 1)), data
        
        # Create lagged features
        Y = data[self.ar_order:]
        X = np.zeros((n - self.ar_order, self.ar_order + 1))
        X[:, 0] = 1  # Intercept
        
        for lag in range(1, self.ar_order + 1):
            X[:, lag] = data[self.ar_order - lag:-lag if lag > 0 else None]
        
        return X, Y
    
    def _initialize_parameters(self, X: np.ndarray, Y: np.ndarray) -> None:
        """Initialize parameters."""
        n = len(Y)
        
        # Transition matrix (favor staying in regime)
        self.transition_matrix = np.ones((self.n_regimes, self.n_regimes))
        np.fill_diagonal(self.transition_matrix, 3)
        self.transition_matrix /= self.transition_matrix.sum(axis=1, keepdims=True)
        
        # Initialize regime-specific parameters using percentile splits
        sorted_indices = np.argsort(Y)
        regime_size = n // self.n_regimes
        
        self.betas = np.zeros((self.n_regimes, X.shape[1]))
        self.sigmas = np.zeros(self.n_regimes)
        
        for k in range(self.n_regimes):
            start = k * regime_size
            end = (k + 1) * regime_size if k < self.n_regimes - 1 else n
            idx = sorted_indices[start:end]
            
            # OLS for this regime
            X_k, Y_k = X[idx], Y[idx]
            try:
                self.betas[k] = np.linalg.lstsq(X_k, Y_k, rcond=None)[0]
            except:
                self.betas[k, 0] = Y_k.mean()
            
            residuals = Y_k - X_k @ self.betas[k]
            self.sigmas[k] = max(np.std(residuals), 0.01)
        
        # Sort regimes by mean
        means = self.betas[:, 0]
        order = np.argsort(means)
        self.betas = self.betas[order]
        self.sigmas = self.sigmas[order]
    
    def _log_likelihood_contribution(
        self,
        X: np.ndarray,
        Y: np.ndarray,
        regime: int
    ) -> np.ndarray:
        """Compute log-likelihood contribution for each observation in a regime."""
        residuals = Y - X @ self.betas[regime]
        ll = (-0.5 * np.log(2 * np.pi * self.sigmas[regime]**2)
              - 0.5 * residuals**2 / self.sigmas[regime]**2)
        return ll
    
    def fit(self, data: np.ndarray) -> 'MarkovSwitchingRegression':
        """
        Fit the Markov Switching model using EM algorithm.
        
        Parameters
        ----------
        data : np.ndarray
            Time series data
            
        Returns
        -------
        self
        """
        data = np.asarray(data).flatten()
        X, Y = self._build_design_matrix(data)
        n = len(Y)
        
        self._initialize_parameters(X, Y)
        
        prev_ll = -np.inf
        
        for iteration in range(self.max_iter):
            # E-step: Compute filtered and smoothed probabilities
            log_emission = np.zeros((n, self.n_regimes))
            for k in range(self.n_regimes):
                log_emission[:, k] = self._log_likelihood_contribution(X, Y, k)
            
            # Hamilton filter (forward)
            log_trans = np.log(self.transition_matrix + 1e-10)
            log_filtered = np.zeros((n, self.n_regimes))
            
            # Initialize with ergodic distribution
            eigvals, eigvecs = np.linalg.eig(self.transition_matrix.T)
            ergodic = np.abs(eigvecs[:, np.argmax(eigvals)])
            ergodic /= ergodic.sum()
            
            log_filtered[0] = np.log(ergodic + 1e-10) + log_emission[0]
            log_filtered[0] -= logsumexp(log_filtered[0])
            
            for t in range(1, n):
                for j in range(self.n_regimes):
                    log_filtered[t, j] = logsumexp(
                        log_filtered[t-1] + log_trans[:, j]
                    ) + log_emission[t, j]
                log_filtered[t] -= logsumexp(log_filtered[t])
            
            # Backward smoother
            log_smoothed = np.zeros((n, self.n_regimes))
            log_smoothed[-1] = log_filtered[-1]
            
            for t in range(n - 2, -1, -1):
                for i in range(self.n_regimes):
                    temp = log_trans[i, :] + log_smoothed[t+1]
                    temp -= logsumexp(log_filtered[t] + log_trans[:, :], axis=0)
                    log_smoothed[t, i] = log_filtered[t, i] + logsumexp(temp)
                log_smoothed[t] -= logsumexp(log_smoothed[t])
            
            smoothed = np.exp(log_smoothed)
            
            # Compute log-likelihood
            log_likelihood = 0
            for t in range(n):
                log_likelihood += logsumexp(
                    log_emission[t] + np.log(smoothed[t] + 1e-10)
                )
            
            # Check convergence
            if abs(log_likelihood - prev_ll) < self.tol:
                break
            prev_ll = log_likelihood
            
            # M-step: Update parameters
            # Update transition matrix
            xi = np.zeros((n - 1, self.n_regimes, self.n_regimes))
            for t in range(n - 1):
                for i in range(self.n_regimes):
                    for j in range(self.n_regimes):
                        xi[t, i, j] = (smoothed[t, i] * self.transition_matrix[i, j] * 
                                      np.exp(log_emission[t+1, j]) * 
                                      smoothed[t+1, j] / (smoothed[t+1, j] + 1e-10))
                xi[t] /= xi[t].sum() + 1e-10
            
            for i in range(self.n_regimes):
                for j in range(self.n_regimes):
                    self.transition_matrix[i, j] = xi[:, i, j].sum() / smoothed[:-1, i].sum()
            self.transition_matrix /= self.transition_matrix.sum(axis=1, keepdims=True)
            
            # Update betas and sigmas
            for k in range(self.n_regimes):
                weights = smoothed[:, k]
                
                # Weighted least squares
                W = np.diag(weights)
                try:
                    self.betas[k] = np.linalg.solve(X.T @ W @ X + 1e-6 * np.eye(X.shape[1]), 
                                                    X.T @ W @ Y)
                except:
                    pass
                
                residuals = Y - X @ self.betas[k]
                self.sigmas[k] = np.sqrt((weights * residuals**2).sum() / weights.sum())
                self.sigmas[k] = max(self.sigmas[k], 0.01)
        
        self._fitted = True
        self._log_likelihood = log_likelihood
        self._n_obs = n
        self._smoothed_probs = smoothed
        self._X = X
        self._Y = Y
        
        return self
    
    def predict_regimes(self, data: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Predict most likely regime sequence.
        
        Parameters
        ----------
        data : np.ndarray, optional
            New data. If None, uses training data.
            
        Returns
        -------
        np.ndarray
            Most likely regime at each time
        """
        if not self._fitted:
            raise ValueError("Model must be fitted first")
        
        if data is None:
            return np.argmax(self._smoothed_probs, axis=1)
        
        # Would need to run filter on new data
        raise NotImplementedError("Prediction on new data not yet implemented")
    
    def get_regime_probabilities(self) -> np.ndarray:
        """Get smoothed regime probabilities."""
        if not self._fitted:
            raise ValueError("Model must be fitted first")
        return self._smoothed_probs
    
    def get_summary(self) -> Dict:
        """Get model summary."""
        if not self._fitted:
            raise ValueError("Model must be fitted first")
        
        n_params = (self.n_regimes * (self.n_regimes - 1) +  # Transition matrix
                   self.n_regimes * (self._X.shape[1] + 1))   # Betas and sigmas
        
        aic = -2 * self._log_likelihood + 2 * n_params
        bic = -2 * self._log_likelihood + n_params * np.log(self._n_obs)
        
        return {
            'n_regimes': self.n_regimes,
            'ar_order': self.ar_order,
            'transition_matrix': self.transition_matrix,
            'regime_means': self.betas[:, 0],
            'regime_ar_coeffs': self.betas[:, 1:] if self.ar_order > 0 else None,
            'regime_volatilities': self.sigmas,
            'log_likelihood': self._log_likelihood,
            'aic': aic,
            'bic': bic,
            'regime_durations': 1 / (1 - np.diag(self.transition_matrix))
        }


class RegimeSwitchingLens:
    """
    Regime Switching Models Lens for comprehensive regime analysis.
    
    Combines HMM and Markov Switching approaches to identify
    and characterize distinct states in time series data.
    """
    
    def __init__(self):
        """Initialize the Regime Switching Lens."""
        self.hmm = None
        self.ms_model = None
    
    def fit_hmm(
        self,
        data: np.ndarray,
        n_states: int = 2,
        **kwargs
    ) -> HMMResult:
        """
        Fit Hidden Markov Model.
        
        Parameters
        ----------
        data : np.ndarray
            Time series data
        n_states : int
            Number of hidden states
            
        Returns
        -------
        HMMResult
            HMM analysis results
        """
        self.hmm = HiddenMarkovModel(n_states=n_states, **kwargs)
        self.hmm.fit(data)
        return self.hmm.get_results(data)
    
    def fit_markov_switching(
        self,
        data: np.ndarray,
        n_regimes: int = 2,
        ar_order: int = 1,
        **kwargs
    ) -> Dict:
        """
        Fit Markov Switching model.
        
        Parameters
        ----------
        data : np.ndarray
            Time series data
        n_regimes : int
            Number of regimes
        ar_order : int
            AR order
            
        Returns
        -------
        dict
            Model summary and predictions
        """
        self.ms_model = MarkovSwitchingRegression(
            n_regimes=n_regimes,
            ar_order=ar_order,
            **kwargs
        )
        self.ms_model.fit(data)
        
        summary = self.ms_model.get_summary()
        summary['regimes'] = self.ms_model.predict_regimes()
        summary['regime_probabilities'] = self.ms_model.get_regime_probabilities()
        
        return summary
    
    def select_optimal_states(
        self,
        data: np.ndarray,
        max_states: int = 5,
        criterion: str = 'bic'
    ) -> Dict:
        """
        Find optimal number of states using information criteria.
        
        Parameters
        ----------
        data : np.ndarray
            Time series data
        max_states : int
            Maximum number of states to test
        criterion : str
            'aic' or 'bic'
            
        Returns
        -------
        dict
            Optimal number of states and all scores
        """
        scores = {}
        
        for n in range(2, max_states + 1):
            hmm = HiddenMarkovModel(n_states=n)
            try:
                hmm.fit(data)
                result = hmm.get_results(data)
                scores[n] = result.aic if criterion == 'aic' else result.bic
            except:
                scores[n] = np.inf
        
        optimal = min(scores, key=scores.get)
        
        return {
            'optimal_states': optimal,
            'scores': scores,
            'criterion': criterion
        }
    
    def analyze(
        self,
        data: Union[np.ndarray, pd.Series],
        n_states: int = 2,
        include_ms: bool = True,
        ar_order: int = 1
    ) -> Dict:
        """
        Comprehensive regime analysis.
        
        Parameters
        ----------
        data : array-like
            Time series data
        n_states : int
            Number of states/regimes
        include_ms : bool
            Include Markov Switching analysis
        ar_order : int
            AR order for Markov Switching
            
        Returns
        -------
        dict
            Complete regime analysis
        """
        if isinstance(data, pd.Series):
            data = data.values
        data = np.asarray(data).flatten()
        
        results = {
            'n_observations': len(data),
            'n_states': n_states
        }
        
        # HMM analysis
        hmm_result = self.fit_hmm(data, n_states)
        results['hmm'] = {
            'states': hmm_result.states,
            'state_probs': hmm_result.state_probs,
            'transition_matrix': hmm_result.transition_matrix,
            'state_means': hmm_result.state_means,
            'state_volatilities': np.sqrt(hmm_result.state_variances),
            'log_likelihood': hmm_result.log_likelihood,
            'aic': hmm_result.aic,
            'bic': hmm_result.bic
        }
        
        # Regime statistics
        states = hmm_result.states
        regime_stats = {}
        for k in range(n_states):
            mask = states == k
            regime_stats[k] = {
                'count': mask.sum(),
                'proportion': mask.mean(),
                'empirical_mean': data[mask].mean() if mask.any() else np.nan,
                'empirical_std': data[mask].std() if mask.any() else np.nan
            }
        results['regime_statistics'] = regime_stats
        
        # Regime durations
        durations = {k: [] for k in range(n_states)}
        current_state = states[0]
        current_duration = 1
        
        for i in range(1, len(states)):
            if states[i] == current_state:
                current_duration += 1
            else:
                durations[current_state].append(current_duration)
                current_state = states[i]
                current_duration = 1
        durations[current_state].append(current_duration)
        
        results['regime_durations'] = {
            k: {
                'mean': np.mean(v) if v else np.nan,
                'median': np.median(v) if v else np.nan,
                'max': max(v) if v else np.nan,
                'count': len(v)
            }
            for k, v in durations.items()
        }
        
        # Markov Switching (optional)
        if include_ms:
            try:
                ms_result = self.fit_markov_switching(data, n_states, ar_order)
                results['markov_switching'] = ms_result
            except Exception as e:
                results['markov_switching'] = {'error': str(e)}
        
        return results


# Convenience function
def regime_analysis(
    data: np.ndarray,
    n_states: int = 2,
    include_ms: bool = True
) -> Dict:
    """
    Quick regime switching analysis.
    
    Parameters
    ----------
    data : np.ndarray
        Time series data
    n_states : int
        Number of regimes
    include_ms : bool
        Include Markov Switching model
        
    Returns
    -------
    dict
        Regime analysis results
    """
    lens = RegimeSwitchingLens()
    return lens.analyze(data, n_states, include_ms)


if __name__ == "__main__":
    # Example usage
    np.random.seed(42)
    
    # Generate regime-switching data
    n = 500
    true_states = np.zeros(n, dtype=int)
    data = np.zeros(n)
    
    # Regime parameters
    means = [-1.0, 1.5]
    stds = [0.5, 1.0]
    
    current_state = 0
    for i in range(n):
        # State transition
        if np.random.random() < 0.05:  # 5% chance of switching
            current_state = 1 - current_state
        true_states[i] = current_state
        data[i] = means[current_state] + stds[current_state] * np.random.randn()
    
    # Analyze
    lens = RegimeSwitchingLens()
    results = lens.analyze(data, n_states=2, include_ms=True, ar_order=1)
    
    print("Regime Switching Analysis Complete")
    print(f"\nHMM Results:")
    print(f"  State means: {results['hmm']['state_means']}")
    print(f"  State volatilities: {results['hmm']['state_volatilities']}")
    print(f"  AIC: {results['hmm']['aic']:.2f}")
    print(f"  BIC: {results['hmm']['bic']:.2f}")
    
    print(f"\nRegime Statistics:")
    for k, stats in results['regime_statistics'].items():
        print(f"  Regime {k}: {stats['proportion']*100:.1f}% of observations, "
              f"mean={stats['empirical_mean']:.2f}, std={stats['empirical_std']:.2f}")
    
    print(f"\nRegime Durations:")
    for k, dur in results['regime_durations'].items():
        print(f"  Regime {k}: mean={dur['mean']:.1f}, max={dur['max']}")
    
    # Accuracy vs true states
    predicted = results['hmm']['states']
    accuracy = max(
        (predicted == true_states).mean(),
        (predicted != true_states).mean()  # In case states are flipped
    )
    print(f"\nAccuracy vs true states: {accuracy*100:.1f}%")
